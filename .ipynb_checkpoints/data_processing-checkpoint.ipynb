{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9087223-531d-4337-8fd7-85702911b7ea",
   "metadata": {},
   "source": [
    "# Data processing stage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dcac1ca-7a3b-4f11-87d9-c21d404fc1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07845e60-d21b-49f4-b5ea-e9baf268fe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    \"\"\" Reads a vocabulary from a .txt file into a list of strings\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        a path to a vocabulary file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    review : a list of raw strings\n",
    "        a list of raw strings from a .csv file\n",
    "    sentiment: a list of 1 and 0, where 1 is positive and 0 is negative sentiment\n",
    "    \"\"\"\n",
    "    review = []\n",
    "    # sentiment = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    with open(file_name, newline='', encoding=\"utf8\") as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            review.append(row[0].replace('<br /><br />', ' '))\n",
    "            # review.append(row[0])\n",
    "            # if row[-1] == \"positive\":\n",
    "            #     sentiment.append(1)\n",
    "            # else:\n",
    "            #     sentiment.append(0)\n",
    "                \n",
    "    return review[1:] #, sentiment[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cafabc9-ff9d-4c63-8dad-1c46989cccb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No one expects the Star Trek movies to be high art, but the fans do expect a movie that is as good as some of the best episodes. Unfortunately, this movie had a muddled, implausible plot that just left me cringing - this is by far the worst of the nine (so far) movies. Even the chance to watch the well known characters interact in another movie can't save this movie - including the goofy scenes with Kirk, Spock and McCoy at Yosemite. I would say this movie is not worth a rental, and hardly worth watching, however for the True Fan who needs to see all the movies, renting this movie is about the only way you'll see it - even the cable channels avoid this movie.\n"
     ]
    }
   ],
   "source": [
    "# review, sentiment = read_data(\"toy_dataset.csv\")\n",
    "review = read_data(\"IMDB Dataset.csv\")\n",
    "print(review[-1])\n",
    "# print(sentiment[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eea677-b047-4599-881c-e1d7fdd3381d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Basic Tokenization (NOT USED -- skip to next section)\n",
    "\n",
    "Different tokenization schemes can lead to different results later on. Here we try two types of tokenization: \n",
    "\n",
    "(1) Regular expression tokenization where\n",
    "- all alpha strings with one hyphen or apostrophe inside (i.e should be able to find \"a-ha\" or \"it's\", but not \"hi--ppo\" or \"44.44\")\n",
    "\n",
    "(2) Penn Treebank tokenizer\n",
    "\n",
    "### Stop words\n",
    "We also remove stop words from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "958ad889-b1ef-4c02-a622-d3cf62018309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re #regular expression usage\n",
    "\n",
    "# # #Example of a regular expression tokenizer:\n",
    "# # regex_tokenizer = re.compile(\"\\w\\w*[-'.]*\\w\\w*|\\S\\w*\")\n",
    "\n",
    "# def tokenize(data, tokenizer):\n",
    "#     \"\"\" Reads a vocabulary from a .txt file into a list of strings\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     data : str\n",
    "#         a list of raw strings\n",
    "#     tokenizer:\n",
    "#         a regular expression re.compile() object\n",
    "        \n",
    "#     Returns\n",
    "#     -------\n",
    "#     tokenized_review : a list of lists - each raw string is tokenized \n",
    "#         and lowercased into a list using the tokenizer\n",
    "\n",
    "#     \"\"\"\n",
    "#     tokenized_review = []\n",
    "    \n",
    "#     for review in data:\n",
    "#         tokenized_review.append(re.findall(tokenizer, review.lower()))\n",
    "    \n",
    "#     return tokenized_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf4b422a-a345-4b0a-8c11-8283b92a132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex_tokenizer = re.compile(\"[a-z][a-z]*[-']?[a-z][a-z]*|[a-z]\")\n",
    "\n",
    "# tokenized_reviews = tokenize(review, regex_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2db880-61fa-45da-b249-e118bec1a000",
   "metadata": {},
   "source": [
    "## Normalization: Tokenization and Lemmatization\n",
    "\n",
    "Here we tokenize and lemmatize our words into its stems with [Stanza](https://stanfordnlp.github.io/stanza/index.html).\n",
    "\n",
    "Here are the steps given the name of a file:\n",
    "1. read it\n",
    "2. tokenize and lemmatize it\n",
    "3. lowercase lemmas\n",
    "4. remove lemmas that are present in a stop word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31fc4030-1dc6-4126-befc-016c7696f37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading English model...\n"
     ]
    }
   ],
   "source": [
    "!pip install -q stanza\n",
    "import stanza\n",
    "print(\"Downloading English model...\")\n",
    "stanza.download('en', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ef7d2cc-bd8e-44a9-8b72-6c4896e9e66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U --user -q spacy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f1369d0-c8c2-4793-88b1-38d19cdcd984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got stopwords\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# import ssl\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "stop_words_english = stopwords.words('english')\n",
    "print(\"Got stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fa8988d-94dc-42d2-8256-53c553473852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_normalize(reviews, stopwords=None, tokenize_with_spacy=True):\n",
    "    \"\"\"Tokenizes, lemmatizes, lowercases and removes stop words if specified\n",
    "    \n",
    "    this function takes in a path to a song, reads the song file,\n",
    "    tokenizes it into words, then lemmatizes and lowercases these words.\n",
    "    finally, stopwords given to the function are removed from the list of song lemmas\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        a path to a text file\n",
    "    stopwords : (optional) list of strings\n",
    "        stopwords that should be removed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    normalized_song : list of strings\n",
    "        a song represented as a list of its lemmas\n",
    "    \"\"\"\n",
    "    # create stanza Pipeline object: first tokenize, then lemmatize\n",
    "#     nlp = stanza.Pipeline(lang='en', processors='tokenize, lemma',  verbose=False)\n",
    "    \n",
    "#     song = open(file_name, 'r').read();\n",
    "#     song_doc = nlp(song);\n",
    "#     normalized_song = []\n",
    "    \n",
    "#     for i, sent in enumerate(song_doc.sentences):\n",
    "#         for word in sent.words:\n",
    "#             lem = word.lemma.lower()\n",
    "#             if lem not in stopwords:\n",
    "#                 normalized_song.append(lem)\n",
    "\n",
    "    stopword_filter = lambda word : True if stopwords==None else word in stopwords \n",
    "    nlp = stanza.Pipeline(lang='en', processors='tokenize,lemma',  verbose=False, tokenize_no_ssplit=True, tokenize_with_spacy=tokenize_with_spacy)\n",
    "\n",
    "    #nlp = stanza.Pipeline(lang='en', processors={'tokenize': '', 'lemma': ''}, verbose=False, tokenize_no_ssplit=True)\n",
    "    print('Pipeline made')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    reviews_lemmatized = []\n",
    "    reviews_lemmatized_checkpoint = []\n",
    "    count = 10001\n",
    "    checkpoint_size = 1000\n",
    "    for review in reviews:\n",
    "        review = nlp(review)\n",
    "        temp = []\n",
    "        for sent in review.sentences:\n",
    "            for word in sent.words:\n",
    "                try:\n",
    "                    lemma_lowered = word.lemma.lower()\n",
    "                    if stopword_filter(lemma_lowered):\n",
    "                        temp.append(lemma_lowered)\n",
    "                except:\n",
    "                    print(f\"Couldn't add {word.text} to lemmatized review. Skip!\")\n",
    "        reviews_lemmatized_checkpoint.append(temp)\n",
    "        # reviews_lemmatized_checkpoint.append([word.lemma.lower() for sent in review.sentences for word in sent.words if stopword_filter(word.lemma.lower())])\n",
    "        # print('Review #: ', count)\n",
    "        if count%checkpoint_size == 0:\n",
    "            pathname = \"dataset_lemmatized/dataset_lemmatized_checkpoint_\" + str(count).zfill(5) + \".json\"\n",
    "            print(\"writing checkpoint into: \" + pathname)\n",
    "            with open(pathname, 'w') as f:\n",
    "                json.dump(reviews_lemmatized_checkpoint, f)\n",
    "            reviews_lemmatized += reviews_lemmatized_checkpoint\n",
    "            reviews_lemmatized_checkpoint = []\n",
    "        count+=1\n",
    "        \n",
    "    reviews_lemmatized += reviews_lemmatized_checkpoint\n",
    "    print(f'Done. Time elapsed: {time.time()-start_time}')\n",
    "\n",
    "    with open(\"dataset_lemmatized/dataset_lemmatized.json\", 'w') as f:\n",
    "        json.dump(reviews_lemmatized, f)\n",
    "\n",
    "    # return reviews_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c84afe7-217b-4377-95ec-c0ddc406fb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline made\n",
      "Couldn't add wookie to lemmatized review. Skip!\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_11000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_12000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_13000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_14000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_15000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_16000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_17000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_18000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_19000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_20000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_21000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_22000.json\n",
      "Couldn't add wookie to lemmatized review. Skip!\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_23000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_24000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_25000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_26000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_27000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_28000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_29000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_30000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_31000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_32000.json\n",
      "Couldn't add \"ACTRESS to lemmatized review. Skip!\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_33000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_34000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_35000.json\n",
      "Couldn't add 足 to lemmatized review. Skip!\n",
      "Couldn't add 足 to lemmatized review. Skip!\n",
      "Couldn't add 足 to lemmatized review. Skip!\n",
      "Couldn't add 足 to lemmatized review. Skip!\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_36000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_37000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_38000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_39000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_40000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_41000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_42000.json\n",
      "Couldn't add CONCENTRATING to lemmatized review. Skip!\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_43000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_44000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_45000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_46000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_47000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_48000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_49000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_50000.json\n",
      "Done. Time elapsed: 10406.570177316666\n"
     ]
    }
   ],
   "source": [
    "# tokenize_and_normalize(reviews, stop_words_english)\n",
    "tokenize_and_normalize(review[10000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8591fa7-ac5b-4711-aef0-13c4e54ea0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # DO NOT USE!!! Reading the .json dataset\n",
    "# reviews_lemmatized = []\n",
    "# for count in range(1000, 11000, 1000):\n",
    "#     pathname = \"dataset_lemmatized/dataset_lemmatized_checkpoint_\" + str(count).zfill(5) + \".json\"\n",
    "#     with open(pathname, 'r') as f:\n",
    "#         reviews_lemmatized += json.load(f)\n",
    "        \n",
    "# with open(\"dataset_lemmatized/dataset_lemmatized.json\", 'r') as f:\n",
    "#     reviews_lemmatized += json.load(f)\n",
    "    \n",
    "# np.size(reviews_lemmatized)\n",
    "# with open(\"dataset_lemmatized/dataset_lemmatized.json\", 'w') as f:\n",
    "#     json.dump(reviews_lemmatized, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0067fbc7-0015-481c-ac60-4bc8ca820c3e",
   "metadata": {},
   "source": [
    "### Normalization - removing non-alphabetical lemmas using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5cf6536a-c4e1-40e9-9d17-4a876daaa86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove lemmas that contain non-word chars\n",
    "import re\n",
    "regex_alpha = re.compile('\\A[a-z]+\\Z')\n",
    "reviews_normalized = [[word for word in review if regex_alpha.match(word)!=None] for review in reviews_lemmatized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a86e52b5-7f41-4e87-99a7-814b12c17753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove lemmas that contain non-word chars\n",
    "with open(\"dataset_lemmatized/dataset_normalized.json\", 'w') as f:\n",
    "     json.dump(reviews_normalized, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef14920a-85f6-4953-9869-f96c244155aa",
   "metadata": {},
   "source": [
    "#### TFI - singleton words removal for decreasing sparsity\n",
    "Implemented TFI stopwords removal from normalized reviews from cell above, where words that only appear once in the corpus are removed. Improves sparsity and accuracy.\n",
    "\n",
    "Saif et al. \"On Stopwords, Filtering and Data Sparsity for Sentiment Analysis of Twitter\" http://www.lrec-conf.org/proceedings/lrec2014/pdf/292_Paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1002c9d6-2c09-4883-8545-be8f53856b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slim_dictionary_TFI(counter):\n",
    "    counter.subtract(counter.keys())\n",
    "    return +counter\n",
    "\n",
    "reviews_vocab_TFI = slim_dictionary_TFI(reviews_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "23a43cd0-47aa-4d27-b5d3-948bcf51dc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocabulary size: 82764 \n",
      "Vocabulary size with singletons removed: 50429\n"
     ]
    }
   ],
   "source": [
    "print(f'Original vocabulary size: {len(reviews_vocab.keys())} \\nVocabulary size with singletons removed: {len(reviews_vocab_TFI.keys())}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "809a0119-14bd-4078-95a5-462acd06ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_normalized_TFI = [[word for word in review if word in reviews_vocab_TFI] for review in reviews_normalized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6c6201a5-704b-4bde-a0db-0c174f50af8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset_lemmatized/dataset_normalized_TFIstopword.json\", 'w') as f:\n",
    "     json.dump(reviews_normalized_TFI, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
