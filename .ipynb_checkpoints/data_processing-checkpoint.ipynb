{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9087223-531d-4337-8fd7-85702911b7ea",
   "metadata": {},
   "source": [
    "# Data processing stage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dcac1ca-7a3b-4f11-87d9-c21d404fc1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07845e60-d21b-49f4-b5ea-e9baf268fe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    \"\"\" Reads a vocabulary from a .txt file into a list of strings\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        a path to a vocabulary file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    review : a list of raw strings\n",
    "        a list of raw strings from a .csv file\n",
    "    sentiment: a list of 1 and 0, where 1 is positive and 0 is negative sentiment\n",
    "    \"\"\"\n",
    "    review = []\n",
    "    sentiment = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    with open(file_name, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            review.append(row[0].replace('<br /><br />', ' '))\n",
    "            # review.append(row[0])\n",
    "            if row[-1] == \"positive\":\n",
    "                sentiment.append(1)\n",
    "            else:\n",
    "                sentiment.append(0)\n",
    "                \n",
    "    return review[1:], sentiment[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cafabc9-ff9d-4c63-8dad-1c46989cccb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me. The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word. It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away. I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "file_name = \"toy_dataset.csv\"\n",
    "review, sentiment = read_data(file_name)\n",
    "# print(review[0])\n",
    "# print(sentiment[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eea677-b047-4599-881c-e1d7fdd3381d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Basic Tokenization (NOT USED -- skip to next section)\n",
    "\n",
    "Different tokenization schemes can lead to different results later on. Here we try two types of tokenization: \n",
    "\n",
    "(1) Regular expression tokenization where\n",
    "- all alpha strings with one hyphen or apostrophe inside (i.e should be able to find \"a-ha\" or \"it's\", but not \"hi--ppo\" or \"44.44\")\n",
    "\n",
    "(2) Penn Treebank tokenizer\n",
    "\n",
    "### Stop words\n",
    "We also remove stop words from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "958ad889-b1ef-4c02-a622-d3cf62018309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re #regular expression usage\n",
    "\n",
    "# # #Example of a regular expression tokenizer:\n",
    "# # regex_tokenizer = re.compile(\"\\w\\w*[-'.]*\\w\\w*|\\S\\w*\")\n",
    "\n",
    "# def tokenize(data, tokenizer):\n",
    "#     \"\"\" Reads a vocabulary from a .txt file into a list of strings\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     data : str\n",
    "#         a list of raw strings\n",
    "#     tokenizer:\n",
    "#         a regular expression re.compile() object\n",
    "        \n",
    "#     Returns\n",
    "#     -------\n",
    "#     tokenized_review : a list of lists - each raw string is tokenized \n",
    "#         and lowercased into a list using the tokenizer\n",
    "\n",
    "#     \"\"\"\n",
    "#     tokenized_review = []\n",
    "    \n",
    "#     for review in data:\n",
    "#         tokenized_review.append(re.findall(tokenizer, review.lower()))\n",
    "    \n",
    "#     return tokenized_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf4b422a-a345-4b0a-8c11-8283b92a132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex_tokenizer = re.compile(\"[a-z][a-z]*[-']?[a-z][a-z]*|[a-z]\")\n",
    "\n",
    "# tokenized_reviews = tokenize(review, regex_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2db880-61fa-45da-b249-e118bec1a000",
   "metadata": {},
   "source": [
    "## Normalization: Tokenization and Lemmatization\n",
    "\n",
    "Here we tokenize and lemmatize our words into its stems with [Stanza](https://stanfordnlp.github.io/stanza/index.html).\n",
    "\n",
    "Here are the steps given the name of a file:\n",
    "1. read it\n",
    "2. tokenize and lemmatize it\n",
    "3. lowercase lemmas\n",
    "4. remove lemmas that are present in a stop word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31fc4030-1dc6-4126-befc-016c7696f37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q stanza\n",
    "import stanza\n",
    "print(\"Downloading English model...\")\n",
    "stanza.download('en', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0ef7d2cc-bd8e-44a9-8b72-6c4896e9e66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q spacy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f1369d0-c8c2-4793-88b1-38d19cdcd984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting stopwords...\n"
     ]
    }
   ],
   "source": [
    "print(\"Getting stopwords...\")\n",
    "import nltk\n",
    "# import ssl\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "stop_words_english = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6fa8988d-94dc-42d2-8256-53c553473852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_normalize(reviews, stopwords=None, tokenize_with_spacy=True):\n",
    "    \"\"\"Tokenizes, lemmatizes, lowercases and removes stop words if specified\n",
    "    \n",
    "    this function takes in a path to a song, reads the song file,\n",
    "    tokenizes it into words, then lemmatizes and lowercases these words.\n",
    "    finally, stopwords given to the function are removed from the list of song lemmas\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        a path to a text file\n",
    "    stopwords : (optional) list of strings\n",
    "        stopwords that should be removed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    normalized_song : list of strings\n",
    "        a song represented as a list of its lemmas\n",
    "    \"\"\"\n",
    "    # create stanza Pipeline object: first tokenize, then lemmatize\n",
    "#     nlp = stanza.Pipeline(lang='en', processors='tokenize, lemma',  verbose=False)\n",
    "    \n",
    "#     song = open(file_name, 'r').read();\n",
    "#     song_doc = nlp(song);\n",
    "#     normalized_song = []\n",
    "    \n",
    "#     for i, sent in enumerate(song_doc.sentences):\n",
    "#         for word in sent.words:\n",
    "#             lem = word.lemma.lower()\n",
    "#             if lem not in stopwords:\n",
    "#                 normalized_song.append(lem)\n",
    "\n",
    "    stopword_filter = lambda word : True if stopwords==None else word in stopwords \n",
    "    nlp = stanza.Pipeline(lang='en', processors='tokenize,lemma',  verbose=False, tokenize_no_ssplit=True, tokenize_with_spacy=tokenize_with_spacy)\n",
    "\n",
    "    #nlp = stanza.Pipeline(lang='en', processors={'tokenize': '', 'lemma': ''}, verbose=False, tokenize_no_ssplit=True)\n",
    "    print('Pipeline made')\n",
    "    start_time = time.time()\n",
    "    reviews_lemmatized = []\n",
    "    for review in reviews:\n",
    "        review = nlp(review)\n",
    "        reviews_lemmatized.append([word.lemma.lower() for sent in review.sentences for word in sent.words if stopword_filter(word.lemma.lower())])\n",
    "    print(f'Time elapsed: {time.time()-start_time}')\n",
    "\n",
    "    return reviews_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c84afe7-217b-4377-95ec-c0ddc406fb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_lemmatized_nostopwords = tokenize_and_normalize(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf6536a-c4e1-40e9-9d17-4a876daaa86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_lemmatized_withstopwords = tokenize_and_normalize(reviews, stop_words_english)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
