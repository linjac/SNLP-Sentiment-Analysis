{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9087223-531d-4337-8fd7-85702911b7ea",
   "metadata": {},
   "source": [
    "# Data processing stage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6dcac1ca-7a3b-4f11-87d9-c21d404fc1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "07845e60-d21b-49f4-b5ea-e9baf268fe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    \"\"\" Reads a vocabulary from a .txt file into a list of strings\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        a path to a vocabulary file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    review : a list of raw strings\n",
    "        a list of raw strings from a .csv file\n",
    "    sentiment: a list of 1 and 0, where 1 is positive and 0 is negative sentiment\n",
    "    \"\"\"\n",
    "    review = []\n",
    "    sentiment = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    with open(file_name, newline='', encoding=\"utf8\") as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            review.append(row[0].replace('<br /><br />', ' '))\n",
    "            review.append(row[0])\n",
    "            if row[-1] == \"positive\":\n",
    "                sentiment.append(1)\n",
    "            else:\n",
    "                sentiment.append(0)\n",
    "                \n",
    "    return review[1:], sentiment[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0cafabc9-ff9d-4c63-8dad-1c46989cccb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No one expects the Star Trek movies to be high art, but the fans do expect a movie that is as good as some of the best episodes. Unfortunately, this movie had a muddled, implausible plot that just left me cringing - this is by far the worst of the nine (so far) movies. Even the chance to watch the well known characters interact in another movie can't save this movie - including the goofy scenes with Kirk, Spock and McCoy at Yosemite.<br /><br />I would say this movie is not worth a rental, and hardly worth watching, however for the True Fan who needs to see all the movies, renting this movie is about the only way you'll see it - even the cable channels avoid this movie.\n"
     ]
    }
   ],
   "source": [
    "# review, sentiment = read_data(\"toy_dataset.csv\")\n",
    "review, sentiment = read_data(\"IMDB Dataset.csv\")\n",
    "print(review[-1])\n",
    "# print(sentiment[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eea677-b047-4599-881c-e1d7fdd3381d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Basic Tokenization (NOT USED -- skip to next section)\n",
    "\n",
    "Different tokenization schemes can lead to different results later on. Here we try two types of tokenization: \n",
    "\n",
    "(1) Regular expression tokenization where\n",
    "- all alpha strings with one hyphen or apostrophe inside (i.e should be able to find \"a-ha\" or \"it's\", but not \"hi--ppo\" or \"44.44\")\n",
    "\n",
    "(2) Penn Treebank tokenizer\n",
    "\n",
    "### Stop words\n",
    "We also remove stop words from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "958ad889-b1ef-4c02-a622-d3cf62018309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re #regular expression usage\n",
    "\n",
    "# # #Example of a regular expression tokenizer:\n",
    "# # regex_tokenizer = re.compile(\"\\w\\w*[-'.]*\\w\\w*|\\S\\w*\")\n",
    "\n",
    "# def tokenize(data, tokenizer):\n",
    "#     \"\"\" Reads a vocabulary from a .txt file into a list of strings\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     data : str\n",
    "#         a list of raw strings\n",
    "#     tokenizer:\n",
    "#         a regular expression re.compile() object\n",
    "        \n",
    "#     Returns\n",
    "#     -------\n",
    "#     tokenized_review : a list of lists - each raw string is tokenized \n",
    "#         and lowercased into a list using the tokenizer\n",
    "\n",
    "#     \"\"\"\n",
    "#     tokenized_review = []\n",
    "    \n",
    "#     for review in data:\n",
    "#         tokenized_review.append(re.findall(tokenizer, review.lower()))\n",
    "    \n",
    "#     return tokenized_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf4b422a-a345-4b0a-8c11-8283b92a132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex_tokenizer = re.compile(\"[a-z][a-z]*[-']?[a-z][a-z]*|[a-z]\")\n",
    "\n",
    "# tokenized_reviews = tokenize(review, regex_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2db880-61fa-45da-b249-e118bec1a000",
   "metadata": {},
   "source": [
    "## Normalization: Tokenization and Lemmatization\n",
    "\n",
    "Here we tokenize and lemmatize our words into its stems with [Stanza](https://stanfordnlp.github.io/stanza/index.html).\n",
    "\n",
    "Here are the steps given the name of a file:\n",
    "1. read it\n",
    "2. tokenize and lemmatize it\n",
    "3. lowercase lemmas\n",
    "4. remove lemmas that are present in a stop word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31fc4030-1dc6-4126-befc-016c7696f37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading English model...\n"
     ]
    }
   ],
   "source": [
    "!pip install -q stanza\n",
    "import stanza\n",
    "print(\"Downloading English model...\")\n",
    "stanza.download('en', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ef7d2cc-bd8e-44a9-8b72-6c4896e9e66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U --user -q spacy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f1369d0-c8c2-4793-88b1-38d19cdcd984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got stopwords\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# import ssl\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "stop_words_english = stopwords.words('english')\n",
    "print(\"Got stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fa8988d-94dc-42d2-8256-53c553473852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_normalize(reviews, stopwords=None, tokenize_with_spacy=True, checkpoint_size=1000):\n",
    "    \"\"\"Tokenizes, lemmatizes, lowercases and removes stop words if specified\n",
    "    \n",
    "    this function takes in a path to a song, reads the song file,\n",
    "    tokenizes it into words, then lemmatizes and lowercases these words.\n",
    "    finally, stopwords given to the function are removed from the list of song lemmas\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        a path to a text file\n",
    "    stopwords : (optional) list of strings\n",
    "        stopwords that should be removed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    normalized_song : list of strings\n",
    "        a song represented as a list of its lemmas\n",
    "    \"\"\"\n",
    "    # create stanza Pipeline object: first tokenize, then lemmatize\n",
    "#     nlp = stanza.Pipeline(lang='en', processors='tokenize, lemma',  verbose=False)\n",
    "    \n",
    "#     song = open(file_name, 'r').read();\n",
    "#     song_doc = nlp(song);\n",
    "#     normalized_song = []\n",
    "    \n",
    "#     for i, sent in enumerate(song_doc.sentences):\n",
    "#         for word in sent.words:\n",
    "#             lem = word.lemma.lower()\n",
    "#             if lem not in stopwords:\n",
    "#                 normalized_song.append(lem)\n",
    "\n",
    "    stopword_filter = lambda word : True if stopwords==None else word in stopwords \n",
    "    nlp = stanza.Pipeline(lang='en', processors='tokenize,lemma',  verbose=False, tokenize_no_ssplit=True, tokenize_with_spacy=tokenize_with_spacy)\n",
    "    print('Pipeline made')\n",
    "    start_time = time.time() # log the time\n",
    "    \n",
    "    reviews_lemmatized = []\n",
    "    reviews_lemmatized_checkpoint = []\n",
    "    count = 0\n",
    "    \n",
    "    for review in reviews:\n",
    "        count+=1\n",
    "        review = nlp(review)\n",
    "        temp = []\n",
    "        for sent in review.sentences:\n",
    "            for word in sent.words:\n",
    "                # sometimes there is an error here, so have a try except\n",
    "                try: \n",
    "                    lemma_lowered = word.lemma.lower()\n",
    "                    if stopword_filter(lemma_lowered):\n",
    "                        temp.append(lemma_lowered)\n",
    "                except:\n",
    "                    print(f\"Couldn't add {word.text} to lemmatized review. Skip!\")\n",
    "        reviews_lemmatized_checkpoint.append(temp)\n",
    "\n",
    "        if count%checkpoint_size == 0:\n",
    "            pathname = \"dataset_lemmatized/dataset_lemmatized_checkpoint_\" + str(count).zfill(5) + \".json\"\n",
    "            print(\"writing checkpoint into: \" + pathname)\n",
    "            with open(pathname, 'w') as f:\n",
    "                json.dump(reviews_lemmatized_checkpoint, f)\n",
    "            reviews_lemmatized += reviews_lemmatized_checkpoint\n",
    "            reviews_lemmatized_checkpoint = []\n",
    "        \n",
    "    reviews_lemmatized += reviews_lemmatized_checkpoint\n",
    "    print(f'Done. Time elapsed: {time.time()-start_time}')\n",
    "\n",
    "    with open(\"dataset_lemmatized/dataset_lemmatized.json\", 'w') as f:\n",
    "        json.dump(reviews_lemmatized, f)\n",
    "\n",
    "    # return reviews_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c84afe7-217b-4377-95ec-c0ddc406fb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline made\n",
      "Couldn't add wookie to lemmatized review. Skip!\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_11000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_12000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_13000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_14000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_15000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_16000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_17000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_18000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_19000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_20000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_21000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_22000.json\n",
      "Couldn't add wookie to lemmatized review. Skip!\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_23000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_24000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_25000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_26000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_27000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_28000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_29000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_30000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_31000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_32000.json\n",
      "Couldn't add \"ACTRESS to lemmatized review. Skip!\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_33000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_34000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_35000.json\n",
      "Couldn't add 足 to lemmatized review. Skip!\n",
      "Couldn't add 足 to lemmatized review. Skip!\n",
      "Couldn't add 足 to lemmatized review. Skip!\n",
      "Couldn't add 足 to lemmatized review. Skip!\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_36000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_37000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_38000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_39000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_40000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_41000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_42000.json\n",
      "Couldn't add CONCENTRATING to lemmatized review. Skip!\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_43000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_44000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_45000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_46000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_47000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_48000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_49000.json\n",
      "writing checkpoint into: dataset_lemmatized/dataset_lemmatized_checkpoint_50000.json\n",
      "Done. Time elapsed: 10406.570177316666\n"
     ]
    }
   ],
   "source": [
    "# tokenize_and_normalize(reviews, stop_words_english)\n",
    "tokenize_and_normalize(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a8591fa7-ac5b-4711-aef0-13c4e54ea0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # DO NOT USE!!! Reading the .json dataset\n",
    "# reviews_lemmatized = []\n",
    "# for count in range(1000, 51000, 1000):\n",
    "#     pathname = \"dataset_lemmatized/dataset_lemmatized_checkpoint_\" + str(count).zfill(5) + \".json\"\n",
    "#     with open(pathname, 'r') as f:\n",
    "#         reviews_lemmatized = reviews_lemmatized + json.load(f)\n",
    "        \n",
    "# # np.size(reviews_lemmatized)\n",
    "# with open(\"dataset_lemmatized/dataset_lemmatized.json\", 'w') as f:\n",
    "#     json.dump(reviews_lemmatized, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0067fbc7-0015-481c-ac60-4bc8ca820c3e",
   "metadata": {},
   "source": [
    "## Normalization - removing non-alphabetical lemmas using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5cf6536a-c4e1-40e9-9d17-4a876daaa86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove lemmas that contain non-word chars\n",
    "import re\n",
    "regex_alpha = re.compile('\\A[a-z]+\\Z')\n",
    "reviews_normalized = [[word for word in review if regex_alpha.match(word)!=None] for review in reviews_lemmatized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a86e52b5-7f41-4e87-99a7-814b12c17753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove lemmas that contain non-word chars\n",
    "with open(\"dataset_lemmatized/dataset_normalized.json\", 'w') as f:\n",
    "     json.dump(reviews_normalized, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd07487-88de-46f5-a32b-34ceb61b8b69",
   "metadata": {},
   "source": [
    "#### Stopwords English - nltk\n",
    "Remove english stopwords from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f4decbb6-f861-4ca2-86ef-a516f8c2efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words_english = stopwords.words('english')\n",
    "\n",
    "reviews_normalized_NLTKstopword = [[word for word in review if word not in stop_words_english] for review in reviews_normalized]\n",
    "with open(\"dataset_lemmatized/dataset_normalized_NLTKstopword.json\", 'w') as f:\n",
    "     json.dump(reviews_normalized_NLTKstopword, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2a5378d3-05d6-4f6f-945a-1e0d3cb9d48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_normalized_NLTKstopword[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef14920a-85f6-4953-9869-f96c244155aa",
   "metadata": {},
   "source": [
    "#### Stopwords Method TF1 - singleton words removal for decreasing sparsity\n",
    "Implemented TF1 stopwords removal from normalized reviews from cell above, where words that only appear once in the corpus are removed. Improves sparsity and accuracy.\n",
    "\n",
    "Saif et al. \"On Stopwords, Filtering and Data Sparsity for Sentiment Analysis of Twitter\" http://www.lrec-conf.org/proceedings/lrec2014/pdf/292_Paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11de1d2-594b-44f1-94ef-6ffc7a123271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0821048c-2fd6-49b3-b2f8-136aeeb7f580",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_vocab = Counter(np.concatenate(reviews_normalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1002c9d6-2c09-4883-8545-be8f53856b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slim_dictionary_TF1(counter):\n",
    "    counter.subtract(counter.keys())\n",
    "    return +counter\n",
    "\n",
    "reviews_vocab_TF1 = slim_dictionary_TF1(reviews_vocab)\n",
    "print(f'Original vocabulary size: {len(reviews_vocab.keys())} \\nVocabulary size with singletons removed: {len(reviews_vocab_TF1.keys())}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "809a0119-14bd-4078-95a5-462acd06ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "### remove only TF1 stopwords\n",
    "reviews_normalized_TF1 = [[word for word in review if word in reviews_vocab_TF1] for review in reviews_normalized]\n",
    "with open(\"dataset_lemmatized/dataset_normalized_TF1stopword.json\", 'w') as f:\n",
    "     json.dump(reviews_normalized_TF1, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bdef21-8e65-4511-8a46-1793241b3be3",
   "metadata": {},
   "source": [
    "Stopwords TF1 + NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2df421b2-98bf-406d-8cb7-9b27a3587bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### remove both TF1 and NLTK stopwords\n",
    "reviews_normalized_NLTKTF1 = [[word for word in review if word in reviews_vocab_TF1] for review in reviews_normalized_NLTKstopword]\n",
    "with open(\"dataset_lemmatized/dataset_normalized_NLTK+TF1stopword.json\", 'w') as f:\n",
    "     json.dump(reviews_normalized_NLTKTF1, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d485db12-4245-438e-bcdf-cddfcef32a35",
   "metadata": {},
   "source": [
    "#### Stopwords Method Mutual Information - removal words from vocab with low mutual information\n",
    "Implemented Mutual Information MI stopwords removal from normalized reviews, where words have low mutual information score, meaning the word is not correlated with sentiment. Use the elbow method to determine cutoff for stopwords ordered by informativeness. -> See paper on informative score I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f9817a03-7c0d-443f-8e83-10fc34c5acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "57c60888-2f6a-4f13-87c6-71ac8ea4bdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MI_score(train_x, train_y, word_freq_cutoff=1):\n",
    "    \"\"\" \n",
    "    takes in a training set of normalized reviews and associated ground truth labels, \n",
    "    calculates mutual information score for the words in the train vocab\n",
    "    \"\"\"\n",
    "    train_vocab = Counter(np.concatenate(train_x))\n",
    "    keys = list(np.repeat(list(train_vocab.keys()),word_freq_cutoff))\n",
    "    train_vocab.subtract(keys)\n",
    "    train_vocab_trimmed = +train_vocab\n",
    "    print(f'Original vocabulary size: {len(train_vocab.keys())} \\nVocabulary size with infrequent words removed: {len(train_vocab_trimmed.keys())}') \n",
    "    \n",
    "    MI = dict()\n",
    "    for word in train_vocab_trimmed.keys():\n",
    "        MI[word] = normalized_mutual_info_score([bool(word in x) for x in train_x], train_y)\n",
    "    \n",
    "    # # plotting\n",
    "    # x,mi_score = zip(*sorted(MI.items(), key=lambda item: item[1], reverse=True))\n",
    "    # plt.plot(x,mi_score)\n",
    "    # plt.show()\n",
    "    \n",
    "    return MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "56a5643d-cb36-4608-998e-92a3fb9fbdbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['violence', 'violence'], dtype='<U8')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_x = reviews_normalized[0:4]\n",
    "word = [\"violence\"]\n",
    "# y\n",
    "# print()\n",
    "# np.array(sentiment[0:100])\n",
    "# np.shape(reviews_normalized[0:100])\n",
    "np.repeat(word, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b0f4f3ab-17a2-4b8c-b829-094ac3cc0dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocabulary size: 42010 \n",
      "Vocabulary size with singletons removed: 25800\n"
     ]
    }
   ],
   "source": [
    "train_mi = calculate_MI_score(reviews_normalized[0:10000], sentiment[0:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4d82229f-33d4-45df-a26e-68d628d4eae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x,mi_score = zip(*sorted(train_mi.items(), key=lambda item: item[1], reverse=True))\n",
    "plt.plot(x,mi_score)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
