{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    \"\"\" Reads a vocabulary from a .txt file into a list of strings\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        a path to a vocabulary file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    review : a list of raw strings\n",
    "        a list of raw strings from a .csv file\n",
    "    sentiment: a list of 1 and 0, where 1 is positive and 0 is negative sentiment\n",
    "    \"\"\"\n",
    "    review = []\n",
    "    sentiment = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    with open(file_name, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            review.append(row[0].replace('<br /><br />', ' '))\n",
    "            # review.append(row[0])\n",
    "            if row[-1] == \"positive\":\n",
    "                sentiment.append(1)\n",
    "            else:\n",
    "                sentiment.append(0)\n",
    "                \n",
    "    return review[1:], sentiment[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"toy_dataset.csv\"\n",
    "review, sentiment = read_data(file_name)\n",
    "# print(review[0])\n",
    "# print(sentiment[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Tokenization (NOT USED -- skip to next section)\n",
    "\n",
    "Different tokenization schemes can lead to different results later on. Here we try two types of tokenization:\n",
    "\n",
    "(1) Regular expression tokenization where\n",
    "\n",
    "- all alpha strings with one hyphen or apostrophe inside (i.e should be able to find \"a-ha\" or \"it's\", but not \"hi--ppo\" or \"44.44\")\n",
    "\n",
    "(2) Penn Treebank tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words\n",
    "\n",
    "We also remove stop words from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import re #regular expression usage\n",
    "\n",
    "# # #Example of a regular expression tokenizer:\n",
    "# # regex_tokenizer = re.compile(\"\\w\\w*[-'.]*\\w\\w*|\\S\\w*\")\n",
    "\n",
    "# def tokenize(data, tokenizer):\n",
    "#     \"\"\" Reads a vocabulary from a .txt file into a list of strings\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     data : str\n",
    "#         a list of raw strings\n",
    "#     tokenizer:\n",
    "#         a regular expression re.compile() object\n",
    "        \n",
    "#     Returns\n",
    "#     -------\n",
    "#     tokenized_review : a list of lists - each raw string is tokenized \n",
    "#         and lowercased into a list using the tokenizer\n",
    "\n",
    "#     \"\"\"\n",
    "#     tokenized_review = []\n",
    "    \n",
    "#     for review in data:\n",
    "#         tokenized_review.append(re.findall(tokenizer, review.lower()))\n",
    "    \n",
    "#     return tokenized_review\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex_tokenizer = re.compile(\"[a-z][a-z]*[-']?[a-z][a-z]*|[a-z]\")\n",
    "\n",
    "# tokenized_reviews = tokenize(review, regex_tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Normalization: Tokenization and Lemmatization\n",
    "\n",
    "Here we tokenize and lemmatize our words into its stems with Stanza.\n",
    "\n",
    "Here are the steps given the name of a file:\n",
    "\n",
    "- read it\n",
    "- tokenize and lemmatize it\n",
    "- lowercase lemmas\n",
    "- remove lemmas that are present in a stop word list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "!pip install -q stanza\n",
    "import stanza\n",
    "print(\"Downloading English model...\")\n",
    "stanza.download('en', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "!pip install -U -q spacy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting stopwords...\n"
     ]
    }
   ],
   "source": [
    "print(\"Getting stopwords...\")\n",
    "import nltk\n",
    "# import ssl\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "stop_words_english = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_normalize(reviews, stopwords=None, tokenize_with_spacy=True):\n",
    "    \"\"\"Tokenizes, lemmatizes, lowercases and removes stop words if specified\n",
    "    \n",
    "    this function takes in a path to a song, reads the song file,\n",
    "    tokenizes it into words, then lemmatizes and lowercases these words.\n",
    "    finally, stopwords given to the function are removed from the list of song lemmas\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        a path to a text file\n",
    "    stopwords : (optional) list of strings\n",
    "        stopwords that should be removed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    normalized_song : list of strings\n",
    "        a song represented as a list of its lemmas\n",
    "    \"\"\"\n",
    "    # create stanza Pipeline object: first tokenize, then lemmatize\n",
    "#     nlp = stanza.Pipeline(lang='en', processors='tokenize, lemma',  verbose=False)\n",
    "    \n",
    "#     song = open(file_name, 'r').read();\n",
    "#     song_doc = nlp(song);\n",
    "#     normalized_song = []\n",
    "    \n",
    "#     for i, sent in enumerate(song_doc.sentences):\n",
    "#         for word in sent.words:\n",
    "#             lem = word.lemma.lower()\n",
    "#             if lem not in stopwords:\n",
    "#                 normalized_song.append(lem)\n",
    "\n",
    "    stopword_filter = lambda word : True if stopwords==None else word in stopwords \n",
    "    nlp = stanza.Pipeline(lang='en', processors='tokenize,lemma',  verbose=False, tokenize_no_ssplit=True, tokenize_with_spacy=tokenize_with_spacy)\n",
    "\n",
    "    #nlp = stanza.Pipeline(lang='en', processors={'tokenize': '', 'lemma': ''}, verbose=False, tokenize_no_ssplit=True)\n",
    "    print('Pipeline made')\n",
    "    start_time = time.time()\n",
    "    reviews_lemmatized = []\n",
    "    for review in reviews:\n",
    "        review = nlp(review)\n",
    "        reviews_lemmatized.append([word.lemma.lower() for sent in review.sentences for word in sent.words if stopword_filter(word.lemma.lower())])\n",
    "    print(f'Time elapsed: {time.time()-start_time}')\n",
    "\n",
    "    return reviews_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reviews_lemmatized_nostopwords \u001b[39m=\u001b[39m tokenize_and_normalize(reviews)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reviews' is not defined"
     ]
    }
   ],
   "source": [
    "reviews_lemmatized_nostopwords = tokenize_and_normalize(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_lemmatized_withstopwords = tokenize_and_normalize(reviews, stop_words_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
